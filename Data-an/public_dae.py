import pandas as pd
import numpy as np
import re
import jieba
import jieba.posseg as pseg
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import os
import pickle
from collections import Counter

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# è®¾ç½®æ•°æ®è·¯å¾„
DATA_PATH = r'D:\\PythonProject\\data\\'  # æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
RAW_FILE = ("ham_data.csv")  # åŸå§‹æ•°æ®æ–‡ä»¶å
PROCESSED_FILE = "processed_spam_messages.csv"  # å¤„ç†åçš„æ•°æ®æ–‡ä»¶å
FEATURES_FILE = "text_features.pkl"  # ç‰¹å¾æ–‡ä»¶

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(DATA_PATH, exist_ok=True)

print(f"æ•°æ®è·¯å¾„: {os.path.abspath(DATA_PATH)}")


def load_data(file_path):
    """åŠ è½½æ•°æ®æ–‡ä»¶"""
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
        df = None

        for encoding in encodings:
            try:
                df = pd.read_csv(file_path, encoding=encoding)
                print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç åŠ è½½æ•°æ®")
                break
            except UnicodeDecodeError:
                continue

        if df is None:
            raise Exception("æ— æ³•æ‰¾åˆ°åˆé€‚çš„ç¼–ç æ ¼å¼")

        return df
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        print("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œå½“å‰å·¥ä½œç›®å½•:", os.getcwd())
        return None


class ChineseTextPreprocessor:
    def __init__(self, stopwords_path=None):
        """åˆå§‹åŒ–ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†å™¨"""
        self.stopwords = set()

        # åŠ è½½åœç”¨è¯è¡¨
        if stopwords_path and os.path.exists(stopwords_path):
            self.load_stopwords(stopwords_path)
        else:
            # ä½¿ç”¨å†…ç½®çš„å¸¸è§ä¸­æ–‡åœç”¨è¯
            self.load_builtin_stopwords()

        # åˆå§‹åŒ–jieba
        jieba.initialize()

    def load_stopwords(self, file_path):
        """ä»æ–‡ä»¶åŠ è½½åœç”¨è¯è¡¨"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                self.stopwords = set([line.strip() for line in f])
            print(f"å·²åŠ è½½ {len(self.stopwords)} ä¸ªåœç”¨è¯")
        except Exception as e:
            print(f"åŠ è½½åœç”¨è¯å¤±è´¥: {e}")
            self.load_builtin_stopwords()

    def load_builtin_stopwords(self):
        """åŠ è½½å†…ç½®å¸¸è§ä¸­æ–‡åœç”¨è¯"""
        common_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°',
            'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬',
            'ä½ ä»¬', 'ä»–ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'è¿™æ ·', 'é‚£æ ·', 'ç„¶å', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ',
            'è™½ç„¶', 'å¯ä»¥', 'åº”è¯¥', 'å·²ç»', 'è¿˜æ˜¯', 'é€šè¿‡', 'è¿›è¡Œ', 'ä»¥åŠ', 'æˆ–è€…', 'å¹¶ä¸”'
        }
        self.stopwords = common_stopwords
        print(f"ä½¿ç”¨å†…ç½®åœç”¨è¯è¡¨ï¼Œå…± {len(self.stopwords)} ä¸ªè¯")

    def clean_chinese_text(self, text):
        """æ¸…æ´—ä¸­æ–‡æ–‡æœ¬"""
        if not isinstance(text, str):
            return ""

        # ç§»é™¤URL
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)

        # ç§»é™¤é‚®ç®±
        text = re.sub(r'\S*@\S*\s?', '', text)

        # ç§»é™¤ç”µè¯å·ç 
        text = re.sub(r'[\+]?[0-9]{1,3}?[-\s]?[0-9]{1,4}?[-\s]?[0-9]{1,4}?[-\s]?[0-9]{1,9}', '', text)

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)

        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def segment_text(self, text, use_stopwords=True, use_pos_filter=False):
        """ä¸­æ–‡åˆ†è¯"""
        text = self.clean_chinese_text(text)

        if use_pos_filter:
            # ä½¿ç”¨è¯æ€§æ ‡æ³¨ï¼Œåªä¿ç•™åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯
            words = pseg.cut(text)
            words = [word for word, flag in words if flag.startswith(('n', 'v', 'a'))]
        else:
            # æ™®é€šåˆ†è¯
            words = jieba.cut(text)

        # è¿‡æ»¤åœç”¨è¯
        if use_stopwords:
            words = [word for word in words if word not in self.stopwords and len(word) > 1]

        return ' '.join(words)

    def preprocess_dataframe(self, df, text_column='message'):
        """é¢„å¤„ç†æ•´ä¸ªDataFrame"""
        print("å¼€å§‹ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†...")

        # åˆ›å»ºæ¸…æ´—åçš„æ–‡æœ¬åˆ—
        df['cleaned_text'] = df[text_column].apply(
            lambda x: self.segment_text(x, use_stopwords=True, use_pos_filter=False)
        )

        # ç»Ÿè®¡ä¿¡æ¯
        original_samples = len(df)
        df = df[df['cleaned_text'].str.len() > 0]  # ç§»é™¤ç©ºæ–‡æœ¬
        cleaned_samples = len(df)

        print(f"é¢„å¤„ç†å®Œæˆ: {original_samples} -> {cleaned_samples} ä¸ªæ ·æœ¬")
        print(f"è¿‡æ»¤äº† {original_samples - cleaned_samples} ä¸ªç©ºæ–‡æœ¬")

        return df


def explore_chinese_data(df, text_column='message', label_column='label'):
    """æ¢ç´¢ä¸­æ–‡æ•°æ®ç‰¹å¾"""
    print("=== æ•°æ®æ¢ç´¢ ===")

    # åŸºæœ¬ç»Ÿè®¡
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ:\n{df[label_column].value_counts()}")

    # æ–‡æœ¬é•¿åº¦åˆ†æ
    df['text_length'] = df[text_column].str.len()
    df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))

    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # æ ‡ç­¾åˆ†å¸ƒ
    df[label_column].value_counts().plot(kind='bar', ax=axes[0, 0])
    axes[0, 0].set_title('æ ‡ç­¾åˆ†å¸ƒ')

    # æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
    df['text_length'].hist(bins=50, ax=axes[0, 1])
    axes[0, 1].set_title('åŸå§‹æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')

    # è¯æ±‡æ•°é‡åˆ†å¸ƒ
    df['word_count'].hist(bins=50, ax=axes[1, 0])
    axes[1, 0].set_title('æ¸…æ´—åè¯æ±‡æ•°é‡åˆ†å¸ƒ')

    # æ ‡ç­¾é—´çš„æ–‡æœ¬é•¿åº¦æ¯”è¾ƒ
    df.boxplot(column='text_length', by=label_column, ax=axes[1, 1])
    axes[1, 1].set_title('å„æ ‡ç­¾æ–‡æœ¬é•¿åº¦æ¯”è¾ƒ')

    plt.suptitle('æ•°æ®æ¢ç´¢åˆ†æ')
    plt.tight_layout()
    plt.show()

    # æ–‡æœ¬ç»Ÿè®¡
    print(f"\næ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
    print(df['text_length'].describe())
    print(f"\nè¯æ±‡æ•°é‡ç»Ÿè®¡:")
    print(df['word_count'].describe())

    return df


def chinese_text_feature_analysis(df, text_column='cleaned_text'):
    """æ‰§è¡Œä¸­æ–‡æ–‡æœ¬ç‰¹å¾åˆ†æ - ä¿®å¤åçš„å‡½æ•°"""
    print("å¼€å§‹ä¸­æ–‡æ–‡æœ¬ç‰¹å¾åˆ†æ...")

    # 1. åŸºæœ¬ç»Ÿè®¡
    print("\n=== åŸºæœ¬æ–‡æœ¬ç»Ÿè®¡ ===")
    df['text_length'] = df[text_column].str.len()
    df['word_count'] = df[text_column].apply(lambda x: len(str(x).split()))

    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {df['text_length'].mean():.2f}")
    print(f"å¹³å‡è¯æ±‡æ•°é‡: {df['word_count'].mean():.2f}")
    print(f"æœ€å¤§æ–‡æœ¬é•¿åº¦: {df['text_length'].max()}")
    print(f"æœ€å°æ–‡æœ¬é•¿åº¦: {df['text_length'].min()}")

    # 2. è¯æ±‡åˆ†æ
    print("\n=== è¯æ±‡åˆ†æ ===")
    all_words = []
    for text in df[text_column]:
        if isinstance(text, str):
            words = text.split()
            all_words.extend(words)

    word_freq = Counter(all_words)
    print(f"æ€»è¯æ±‡æ•°: {len(all_words)}")
    print(f"ç‹¬ç‰¹è¯æ±‡æ•°: {len(word_freq)}")
    print(f"æœ€å¸¸å‡ºç°çš„10ä¸ªè¯æ±‡:")
    for word, count in word_freq.most_common(10):
        print(f"  {word}: {count}æ¬¡")

    # 3. å¯è§†åŒ–
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plt.figure(figsize=(15, 5))

    # æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
    plt.subplot(1, 3, 1)
    plt.hist(df['text_length'], bins=30, alpha=0.7)
    plt.xlabel('æ–‡æœ¬é•¿åº¦')
    plt.ylabel('é¢‘æ¬¡')
    plt.title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')

    # è¯æ±‡æ•°é‡åˆ†å¸ƒ
    plt.subplot(1, 3, 2)
    plt.hist(df['word_count'], bins=30, alpha=0.7, color='orange')
    plt.xlabel('è¯æ±‡æ•°é‡')
    plt.ylabel('é¢‘æ¬¡')
    plt.title('è¯æ±‡æ•°é‡åˆ†å¸ƒ')

    # è¯é¢‘åˆ†å¸ƒ
    plt.subplot(1, 3, 3)
    top_words = [word for word, count in word_freq.most_common(20)]
    top_counts = [count for word, count in word_freq.most_common(20)]
    plt.barh(range(len(top_words)), top_counts)
    plt.yticks(range(len(top_words)), top_words)
    plt.xlabel('å‡ºç°é¢‘æ¬¡')
    plt.title('å‰20ä¸ªé«˜é¢‘è¯æ±‡')

    plt.tight_layout()
    plt.show()

    print("ä¸­æ–‡æ–‡æœ¬ç‰¹å¾åˆ†æå®Œæˆ!")


def vectorize_chinese_text(df, text_column='cleaned_text'):
    """ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–"""
    print("=== æ–‡æœ¬å‘é‡åŒ– ===")

    # 1. TF-IDF å‘é‡åŒ–
    tfidf_vectorizer = TfidfVectorizer(
        max_features=5000,
        min_df=2,
        max_df=0.95,
        ngram_range=(1, 2),  # åŒ…å«å•ä¸ªè¯å’ŒåŒè¯ç»„åˆ
        token_pattern=r'(?u)\b\w+\b'  # ä¸­æ–‡åˆ†è¯åå·²ç»æ˜¯ç©ºæ ¼åˆ†éš”
    )

    tfidf_features = tfidf_vectorizer.fit_transform(df[text_column])
    print(f"TF-IDFç‰¹å¾çŸ©é˜µå½¢çŠ¶: {tfidf_features.shape}")

    # 2. è¯è¢‹æ¨¡å‹
    count_vectorizer = CountVectorizer(
        max_features=3000,
        ngram_range=(1, 2)
    )

    count_features = count_vectorizer.fit_transform(df[text_column])
    print(f"è¯è¢‹æ¨¡å‹ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {count_features.shape}")

    # è·å–ç‰¹å¾åç§°
    feature_names = tfidf_vectorizer.get_feature_names_out()
    print(f"\nå‰20ä¸ªTF-IDFç‰¹å¾: {feature_names[:20]}")

    return {
        'tfidf_features': tfidf_features,
        'count_features': count_features,
        'tfidf_vectorizer': tfidf_vectorizer,
        'count_vectorizer': count_vectorizer
    }


def generate_wordcloud(df, label_column='label', text_column='cleaned_text'):
    """ç”Ÿæˆè¯äº‘å›¾"""
    print("=== ç”Ÿæˆè¯äº‘ ===")

    labels = df[label_column].unique()

    fig, axes = plt.subplots(1, len(labels), figsize=(15, 6))
    if len(labels) == 1:
        axes = [axes]

    for i, label in enumerate(labels):
        # è·å–è¯¥æ ‡ç­¾ä¸‹çš„æ‰€æœ‰æ–‡æœ¬
        text_data = ' '.join(df[df[label_column] == label][text_column])

        if len(text_data) > 0:
            # ç”Ÿæˆè¯äº‘
            wordcloud = WordCloud(
                font_path='simhei.ttf',  # ä¸­æ–‡å­—ä½“
                width=400,
                height=300,
                background_color='white',
                max_words=100
            ).generate(text_data)

            axes[i].imshow(wordcloud, interpolation='bilinear')
            axes[i].set_title(f'"{label}" ç±»è¯äº‘')
            axes[i].axis('off')
        else:
            axes[i].text(0.5, 0.5, f'"{label}" ç±»æ— æ•°æ®',
                         ha='center', va='center', transform=axes[i].transAxes)
            axes[i].axis('off')

    plt.tight_layout()
    plt.show()


def save_processed_data(df, text_features, save_dir=DATA_PATH):
    """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
    print("=== ä¿å­˜å¤„ç†ç»“æœ ===")

    # 1. ä¿å­˜å¤„ç†åçš„DataFrame
    processed_file = os.path.join(save_dir, PROCESSED_FILE)
    df.to_csv(processed_file, index=False, encoding='utf-8')
    print(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {processed_file}")

    # 2. ä¿å­˜ç‰¹å¾å’Œå‘é‡åŒ–å™¨
    features_file = os.path.join(save_dir, FEATURES_FILE)
    with open(features_file, 'wb') as f:
        pickle.dump(text_features, f)
    print(f"ç‰¹å¾æ•°æ®å·²ä¿å­˜: {features_file}")

    # 3. ä¿å­˜é¢„å¤„ç†æŠ¥å‘Š
    report_file = os.path.join(save_dir, "preprocessing_report.txt")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("ä¸­æ–‡åƒåœ¾çŸ­ä¿¡æ•°æ®é¢„å¤„ç†æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n")
        f.write(f"å¤„ç†æ—¶é—´: {pd.Timestamp.now()}\n")
        f.write(f"åŸå§‹æ ·æœ¬æ•°: {len(df)}\n")
        f.write(f"å¤„ç†åæ ·æœ¬æ•°: {len(df)}\n")
        f.write(f"ç‰¹å¾ç»´åº¦: {text_features['tfidf_features'].shape[1]}\n")
        f.write(f"æ ‡ç­¾åˆ†å¸ƒ:\n{df['label'].value_counts().to_string()}\n")

    print(f"é¢„å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return {
        'processed_data': processed_file,
        'features': features_file,
        'report': report_file
    }


def inspect_processed_results(saved_files):
    """æŸ¥çœ‹å¤„ç†ç»“æœ"""
    print("\n=== å¦‚ä½•æŸ¥çœ‹å¤„ç†ç»“æœ ===")

    for file_type, file_path in saved_files.items():
        print(f"\n{file_type.upper()} æ–‡ä»¶: {file_path}")

        if os.path.exists(file_path):
            if file_type == 'processed_data':
                # æŸ¥çœ‹å¤„ç†åçš„æ•°æ®
                df_processed = pd.read_csv(file_path, encoding='utf-8')
                print("å¤„ç†åçš„æ•°æ®å‰5è¡Œ:")
                print(df_processed[['message', 'cleaned_text', 'label']].head())

            elif file_type == 'report':
                # æŸ¥çœ‹æŠ¥å‘Š
                with open(file_path, 'r', encoding='utf-8') as f:
                    print("é¢„å¤„ç†æŠ¥å‘Šå†…å®¹:")
                    print(f.read())

            elif file_type == 'features':
                # æŸ¥çœ‹ç‰¹å¾ä¿¡æ¯
                with open(file_path, 'rb') as f:
                    features = pickle.load(f)
                    print("ç‰¹å¾çŸ©é˜µå½¢çŠ¶:", features['tfidf_features'].shape)
                    print("ç‰¹å¾æ•°é‡:", len(features['tfidf_vectorizer'].get_feature_names_out()))
        else:
            print("æ–‡ä»¶ä¸å­˜åœ¨")


def complete_chinese_spam_preprocessing(data_file, text_column='message', label_column='label'):
    """å®Œæ•´çš„ä¸­æ–‡åƒåœ¾çŸ­ä¿¡é¢„å¤„ç†æµç¨‹"""

    # 1. åŠ è½½æ•°æ®
    df = load_data(data_file)
    if df is None:
        return None

    # 2. åˆå§‹åŒ–é¢„å¤„ç†å™¨
    preprocessor = ChineseTextPreprocessor()

    # 3. æ–‡æœ¬é¢„å¤„ç†
    df = preprocessor.preprocess_dataframe(df, text_column)

    # 4. æ•°æ®æ¢ç´¢
    df = explore_chinese_data(df, text_column, label_column)

    # 5. ä¸­æ–‡æ–‡æœ¬ç‰¹å¾åˆ†æ - ä¿®å¤åçš„å‡½æ•°è°ƒç”¨
    chinese_text_feature_analysis(df)

    # 6. æ–‡æœ¬å‘é‡åŒ–
    text_features = vectorize_chinese_text(df)

    # 7. å¯è§†åŒ–
    generate_wordcloud(df, label_column)

    # 8. ä¿å­˜ç»“æœ
    saved_files = save_processed_data(df, text_features)

    # 9. æŸ¥çœ‹ç»“æœ
    inspect_processed_results(saved_files)

    return df, text_features, saved_files


# è¿è¡Œå®Œæ•´æµç¨‹
if __name__ == "__main__":
    data_file = os.path.join(DATA_PATH, RAW_FILE)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file):
        print(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print("è¯·ç¡®ä¿å·²å°†æ‚¨çš„CSVæ–‡ä»¶æ”¾ç½®åœ¨æ­£ç¡®è·¯å¾„")
    else:
        results = complete_chinese_spam_preprocessing(data_file)

        if results:
            df_processed, features, files = results
            print("\nğŸ‰ é¢„å¤„ç†å®Œæˆ! æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°æŒ‡å®šç›®å½•")

            # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶è·¯å¾„
            print("\nç”Ÿæˆçš„æ–‡ä»¶:")
            for file_type, file_path in files.items():
                print(f"  {file_type}: {file_path}")
        else:
            print("\nâŒ é¢„å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")